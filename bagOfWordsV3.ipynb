{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goals:\n",
    "1. Create a corpus of all yt video descriptions for scam videos in the data.\n",
    "2. vectorize this.\n",
    "3. Use this to create a regressor capturing the degree to which a video description is \"spam-speaky\" according to ML result (or dichotomous 1-spammy, 0-notSpammy).\n",
    "4.Use this bag-of-words model to generate values for a new variable, \"spam-speakiness\", for every observation in the dataset .\n",
    "5. (optional) repeat this process with a dataset of spam-lookalike videos which warn AGAINST scams and spam and attempt to create another regressor, \"virtue-speakiness\".\n",
    "6. get \"has_numbers\" and \"number_sum\" for numbers in character vars\n",
    "7. multiply-impute missing values b4 machine learning (training data only, drop missing on test!!!!)\n",
    "8. get capital letter frequency\n",
    "9. get emoji frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json \n",
    "import pandas as pd\n",
    "import pymongo\n",
    "#from google.colab import drive\n",
    "#nltk.download('stopwords')\n",
    "from pymongo import MongoClient\n",
    "import socket\n",
    "import urllib.request as urllib2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk  \n",
    "import numpy as np  \n",
    "import random  \n",
    "import string\n",
    "\n",
    "import urllib.request  \n",
    "import re  \n",
    "from string import punctuation\n",
    "from os import listdir\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "import math\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = 'mongodb://localhost:27017'\n",
    "client = pymongo.MongoClient(conn)\n",
    "\n",
    "\n",
    "db = client.videos_mdb\n",
    "collection = db[\"videos\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pull data from mongo (just one iv for now)\n",
    "linVarList=[\"id\",\"spam\",\"description\"]\n",
    "df=pd.DataFrame(columns=linVarList)\n",
    "row=[]\n",
    "counter=0\n",
    "data = collection.find({})\n",
    "for item in data:\n",
    "    for var in linVarList:\n",
    "        if var in item:\n",
    "            row.append(item[var])\n",
    "        else:\n",
    "            row.append(np.nan)\n",
    "    df.loc[counter] = row\n",
    "    counter+=1\n",
    "    row=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam=df.loc[df[\"spam\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create training data from X percent of the total spam entries\n",
    "XPercent=math.floor(len(spam)*0.2)\n",
    "xtrain = spam.sample(n=XPercent, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the rest of the data as testing data\n",
    "xtest=pd.concat([spam, xtrain]).drop_duplicates(keep=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawDescriptions=[(item for item in xtrain[\"description\"])]\n",
    "corpus=\"\"\n",
    "for item in rawDescriptions:\n",
    "    corpus=corpus.join(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    " #nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from emoji import UNICODE_EMOJI\n",
    "\n",
    "def is_emoji(s):\n",
    "    count = 0\n",
    "    for emoji in UNICODE_EMOJI:\n",
    "        count += s.count(emoji)\n",
    "        if count > 1:\n",
    "            return False\n",
    "    return bool(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize.casual import TweetTokenizer\n",
    "import enchant\n",
    "import re\n",
    "t = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TO DO: exclude if is not a word OR is emoji OR is short boring word\n",
    "\n",
    "# turn a doc into clean tokens\n",
    "def clean_doc(doc):\n",
    "\t# split into tokens \n",
    "\ttokens=t.tokenize(doc)\n",
    "\t# remove punctuation from each token\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\ttokens = [w.translate(table) for w in tokens]\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "\t#tokens = [word for word in tokens if word.isalpha() or is_emoji(word)]\n",
    "\t# filter out stop words\n",
    "\tstop_words = set(stopwords.words('english'))\n",
    "\ttokens = [w for w in tokens if not w in stop_words]\n",
    "\t# filter out short tokens\n",
    "\t#tokens = [word for word in tokens if len(word) > 1]\n",
    "\treturn tokens\n",
    "\t\n",
    "\n",
    "# load the document\n",
    "tokens = clean_doc(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordfreq = {}\n",
    "\n",
    "\n",
    "for token in tokens:\n",
    "    if token not in wordfreq.keys():\n",
    "        wordfreq[token.lower()] = 1\n",
    "    else:\n",
    "        wordfreq[token.lower()] += 1\n",
    "\n",
    "exclaim=0\n",
    "question=0\n",
    "hashy=0\n",
    "atme=0\n",
    "\n",
    "for item in corpus:\n",
    "    if item==\"!\":\n",
    "        exclaim+=1\n",
    "    elif item == \"?\":\n",
    "        question+=1\n",
    "    elif item ==\"#\":\n",
    "        hashy+=1\n",
    "\n",
    "\n",
    "wordfreq[\"?\"]=question\n",
    "wordfreq[\"!\"]=exclaim\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = [\",\",\"'\",\".\",\";\",\":\",\"(\",\")\",\"\"]\n",
    "wordfreq.pop(\"\", None)\n",
    "for item in punctuation:\n",
    "    wordfreq.pop(item, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "#most_freq = [i for i in wordfreq if wordfreq[i] >= 1 and i[0:4]!=\"http\" ]\n",
    "#wordfreqmost_freq = heapq.nlargest(200, wordfreq, key=wordfreq.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sorted_freq=sorted(wordfreq.items(), key=lambda x: x[1], reverse=True)\n",
    "most_freq=sorted_freq[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('!', 97),\n",
       " ('‚ñ¨', 90),\n",
       " ('?', 55),\n",
       " ('üëâ', 38),\n",
       " ('‚úÖ', 38),\n",
       " ('üî•', 37),\n",
       " ('‚û°', 37),\n",
       " ('Ô∏è', 37),\n",
       " ('video', 24),\n",
       " ('1', 21),\n",
       " ('always', 18),\n",
       " ('experience', 17),\n",
       " ('cash', 17),\n",
       " ('‚òÖ', 16),\n",
       " ('‚Äô', 16),\n",
       " ('‚ñ∫', 15),\n",
       " ('‚ûú', 14),\n",
       " ('üí•', 12),\n",
       " ('mentioned', 12),\n",
       " ('financial', 12),\n",
       " ('information', 12),\n",
       " ('‚ö´', 12),\n",
       " ('2021', 12),\n",
       " ('200', 12),\n",
       " ('‚Äî', 12),\n",
       " ('üî¥', 10),\n",
       " ('educational', 10),\n",
       " ('purposes', 10),\n",
       " ('guarantee', 10),\n",
       " ('500', 10),\n",
       " ('3', 9),\n",
       " ('üîî', 9),\n",
       " ('risk', 9),\n",
       " ('100', 9),\n",
       " ('300', 9),\n",
       " ('need', 9),\n",
       " ('home', 9),\n",
       " ('typical', 8),\n",
       " ('level', 8),\n",
       " ('2', 8),\n",
       " ('showing', 8),\n",
       " ('5', 7),\n",
       " ('ways', 7),\n",
       " ('üëç', 7),\n",
       " ('advisor', 7),\n",
       " ('advice', 7),\n",
       " ('50', 7),\n",
       " ('achieve', 7),\n",
       " ('videos', 6),\n",
       " ('üëá', 6),\n",
       " ('results', 6),\n",
       " ('personal', 6),\n",
       " ('research', 6),\n",
       " ('steps', 6),\n",
       " ('description', 6),\n",
       " ('product', 6),\n",
       " ('receive', 6),\n",
       " ('income', 6),\n",
       " ('earning', 6),\n",
       " ('400', 6),\n",
       " ('examples', 6),\n",
       " ('ü§ë', 6),\n",
       " ('üí∞', 6),\n",
       " ('products', 6),\n",
       " ('‚óæ', 6),\n",
       " ('techniques', 5),\n",
       " ('say', 5),\n",
       " ('seen', 5),\n",
       " ('success', 5),\n",
       " ('attaining', 5),\n",
       " ('claimed', 5),\n",
       " ('require', 5),\n",
       " ('hardwork', 5),\n",
       " ('knowledge', 5),\n",
       " ('sharing', 5),\n",
       " ('biased', 5),\n",
       " ('opinion', 5),\n",
       " ('based', 5),\n",
       " ('speculation', 5),\n",
       " ('understand', 5),\n",
       " ('investing', 5),\n",
       " ('taken', 5),\n",
       " ('reasonable', 5),\n",
       " ('ensure', 5),\n",
       " ('accurate', 5),\n",
       " ('represent', 5),\n",
       " ('errors', 5),\n",
       " ('links', 5),\n",
       " ('commission', 5),\n",
       " ('fast', 5),\n",
       " ('means', 5),\n",
       " ('well', 5),\n",
       " ('4', 5),\n",
       " ('12', 5),\n",
       " ('strategies', 5),\n",
       " ('support', 5),\n",
       " ('use', 5),\n",
       " ('forget', 5),\n",
       " ('youre', 4),\n",
       " ('‚òÜ', 4),\n",
       " ('14', 4),\n",
       " ('30', 4),\n",
       " ('‚Äú', 4),\n",
       " ('‚Äù', 4),\n",
       " ('earnings', 4),\n",
       " ('one', 4),\n",
       " ('small', 4),\n",
       " ('time', 4),\n",
       " ('think', 4),\n",
       " ('1st', 4),\n",
       " ('2nd', 4),\n",
       " ('statements', 4),\n",
       " ('1402', 4),\n",
       " ('59', 4),\n",
       " ('httpsbransontaycom10xpsyt1', 4),\n",
       " ('517', 4),\n",
       " ('60', 4),\n",
       " ('1000', 4),\n",
       " ('contain', 4),\n",
       " ('provided', 4),\n",
       " ('whether', 4),\n",
       " ('credits', 4),\n",
       " ('also', 4),\n",
       " ('contained', 4),\n",
       " ('download', 4),\n",
       " ('viewing', 4),\n",
       " ('informational', 4),\n",
       " ('\\u200b', 4),\n",
       " ('professional', 4),\n",
       " ('internet', 4),\n",
       " ('üé≠', 4),\n",
       " ('exactly', 4),\n",
       " ('people', 4),\n",
       " ('19', 4),\n",
       " ('hit', 4),\n",
       " ('goals', 4),\n",
       " ('latest', 4),\n",
       " ('tips', 4),\n",
       " ('depth', 4),\n",
       " ('top', 4),\n",
       " ('enjoyed', 4),\n",
       " ('per', 3),\n",
       " ('learn', 3),\n",
       " ('share', 3),\n",
       " ('show', 3),\n",
       " ('important', 3),\n",
       " ('using', 3),\n",
       " ('ideas', 3),\n",
       " ('making', 3),\n",
       " ('investment', 3),\n",
       " ('website', 3),\n",
       " ('cost', 3),\n",
       " ('work', 3),\n",
       " ('enter', 3),\n",
       " ('allows', 3),\n",
       " ('find', 3),\n",
       " ('may', 3),\n",
       " ('extra', 3),\n",
       " ('üí∏', 3),\n",
       " ('3000', 3),\n",
       " ('watch', 3),\n",
       " ('create', 3),\n",
       " ('content', 3),\n",
       " ('made', 3),\n",
       " ('sponsored', 3),\n",
       " ('company', 3),\n",
       " ('put', 3),\n",
       " ('could', 3),\n",
       " ('upon', 3),\n",
       " ('must', 3),\n",
       " ('entire', 3),\n",
       " ('nature', 3),\n",
       " ('want', 3),\n",
       " ('methods', 3),\n",
       " ('talking', 3),\n",
       " ('method', 3),\n",
       " ('community', 3),\n",
       " ('either', 3),\n",
       " ('purchases', 3),\n",
       " ('clicked', 3),\n",
       " ('marketer', 3),\n",
       " ('years', 3),\n",
       " ('httptheblackdogvideocom', 3),\n",
       " ('supportjohncrestanicom', 3),\n",
       " ('cases', 3),\n",
       " ('2000', 3),\n",
       " ('passive', 3),\n",
       " ('started', 3),\n",
       " ('see', 3),\n",
       " ('tutorials', 3),\n",
       " ('looking', 3),\n",
       " ('sure', 3),\n",
       " ('businesses', 3),\n",
       " ('25', 3),\n",
       " ('chat', 3),\n",
       " ('\\u200d', 3),\n",
       " ('subscribers', 3),\n",
       " ('teach', 3),\n",
       " ('‚ù§', 3),\n",
       " ('3rd', 3)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "boringWords=[\"with\",\"that\",\"this\",\"will\",\"your\",\"there\", \"wesleyvirgin\", \"wesleymilliondollarvirgin\",\"just\", \"they\", \"much\",\"kaise\",\"then\", \"than\", \"when\", \"here\",\"these\", \"girl\", \"what\",\"where\",\"from\", \"wesley\", \"virgin\", \"also\",  \"about\", \"them\", \"which\", \"dave\", \"very\"] #\"girl\" is removed b/c comes from name of big scam youtuber and leaving it in risks sexist codings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of words in spamwords/ total number of words = regressor"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "566c7dc836cf49ea79a80bde3a77d731ae67019fab1def2dd83c01090ce827fd"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('DataViz': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}